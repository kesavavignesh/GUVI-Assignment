# -*- coding: utf-8 -*-
"""Final Project E-Commerce customer segmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lVcR66uJlsuEDAHqAPOSNsYQfH2rB-fu

# **About Project**

***E-Commerce customer segmentation***

***

**Target given by mentor**

The end objective of the participant is to come up with customer segmentations that take into account all the information that is presented in the dataset. The participant is expected to use NLP techniques to find similarity between the products.


***

**[ Things I've done ]**


   **In Description column only**

*   Drop Duplicates

*   Natural language processing(NLP)

1.   Remove punctuations
2.   Converting text to lowercase
3.   Removing stop words
4.   Lemmatization
5.   Bag of words



*   K-Means Clustering Algorithm (For NLP processed data)

**In Orginal Data**

*   One hot Encoding (Using "Description's" K-Means values)

*   K-Means Clustering Algorithm (For NLP processed data)

**Importing nessasary packages**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from google.colab import drive
drive.mount('/content/drive')

"""**Getting CSV file**"""

des = pd.read_csv("/content/drive/MyDrive/GUVI Final Project/E-commerce customer segmentation/data.csv", encoding= 'unicode_escape')
des['Description']

des.head()

des.info

des.describe()

des['Description']

"""**Droping duplicates**"""

dforg=des.copy()
des.drop_duplicates('Description',inplace=True)

des['Description']

"""**Droping Missing Values (NaN)**"""

des = des.dropna()

des['Description']

"""# **Natural language processing (NLP)**

**Removing punctuation**
"""

des['Description'] = des['Description'].str.replace("[^a-zA-Z0-9]", " ")
des = des[['Description']]

des['Description']=des['Description'].fillna(" ")

des['Description'] = des['Description'].apply(lambda x: ' '.join([word for word in x.split() if len(word)>2]))

des['Description']

"""**Converting text to lowercase**"""

des['Description'] = [review.lower() for review in des['Description']]

des['Description']

"""**Removing stop words**"""

import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk import word_tokenize
stop_words = stopwords.words('english')

def remove_stopwords(rev):
    review_tokenized = word_tokenize(rev)
    rev_new = " ".join([i for i in review_tokenized  if i not in stop_words])
    return rev_new

des['Description'] = [remove_stopwords(r) for r in des['Description']]

des['Description']

"""**Lemmatization**"""

nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
nltk.download('averaged_perceptron_tagger')


lemmatizer = WordNetLemmatizer()


def nltk_tag_to_wordnet_tag(nltk_tag):
    if nltk_tag.startswith('J'):
        return wordnet.ADJ
    elif nltk_tag.startswith('V'):
        return wordnet.VERB
    elif nltk_tag.startswith('N'):
        return wordnet.NOUN
    elif nltk_tag.startswith('R'):
        return wordnet.ADV
    else:          
        return None


def lemmatize_sentence(sentence):
 
    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  
    
    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)
    lemmatized_sentence = []
    for word, tag in wordnet_tagged:
        if tag is None:
            
            lemmatized_sentence.append(word)
        else:        
           
            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))
    return " ".join(lemmatized_sentence)


des['Description'] = des['Description'].apply(lambda x: lemmatize_sentence(x))

des['Description']

"""**Bag of words**"""

from sklearn.feature_extraction.text import TfidfVectorizer
tfides = TfidfVectorizer(max_features=2500)
des = tfides.fit_transform(des.Description).toarray()
featureNames = tfides.get_feature_names()

des

"""# **K-Means Clustering**

To find unique values
"""

from sklearn.cluster import KMeans
inertias = []
list_k = list(range(1, 20))

for k in list_k:
    km = KMeans(n_clusters=k)
    km.fit(des)
    inertias.append(km.inertia_)

"""Elbow plot"""

from sklearn.cluster import KMeans
inertias = []
list_k = list(range(1, 20))

for k in list_k:
    km = KMeans(n_clusters=k)
    km.fit(des)
    inertias.append(km.inertia_)

dir(KMeans)

cus = KMeans(n_clusters=19,random_state=3)
cus.fit(des)

"""**One hot Encoding**"""

df = pd.read_csv("/content/drive/MyDrive/GUVI Final Project/E-commerce customer segmentation/data.csv", encoding= 'unicode_escape')
df['Description']

df = dforg.copy()
df.drop_duplicates('Description',inplace=True)
df

df = df.dropna()

from sklearn.preprocessing import OneHotEncoder

t = cus.predict(des).tolist()

df["lable"]= t
df

df.drop(df.columns[[0, 1, 2, 3, 4 ,5, 7]], axis = 1, inplace = True)
  
df

df["lable"]= t
df

"""***One hot Encoding***"""

df = pd.get_dummies(df, columns=["lable"], prefix=["C"] )

df

"""# **K-Means Clustering**"""

cus = KMeans(n_clusters=19,random_state=3)
cus.fit(df)

from sklearn.cluster import KMeans
inertias = []
list_k = list(range(1, 20))

for k in list_k:
    km = KMeans(n_clusters=k)
    km.fit(df)
    inertias.append(km.inertia_)

"""Elbow plot"""

import matplotlib.pyplot as plt
plt.figure(figsize=(6, 6))
plt.plot(list_k, inertias, '-o')
plt.xlabel(r'Number of clusters *k*')
plt.ylabel('Inertia');